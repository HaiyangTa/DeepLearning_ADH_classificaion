{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6cb2062-5c05-44c5-a458-4a09e7ad07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from read_eeg import get_eeg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e3a70e-62ac-42ae-9bb1-ec5fa795d867",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/Haiya/Downloads/Data/Data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m num_segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;66;03m# extract 50 segments from each sample!\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# --- File Loading ---\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     10\u001b[0m ad_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ad\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m     11\u001b[0m healthy_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ad\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/Haiya/Downloads/Data/Data/'"
     ]
    }
   ],
   "source": [
    "# --- Parameters ---\n",
    "data_folder = 'C:/Users/Haiya/Downloads/Data/Data/'\n",
    "fs = 500\n",
    "min_samples = int(0.5 * 60 * fs)\n",
    "max_samples = int(1.0 * 60 * fs)\n",
    "num_segments = 50 # extract 50 segments from each sample!\n",
    "\n",
    "# --- File Loading ---\n",
    "files = [f for f in os.listdir(data_folder) if f.endswith('.mat')]\n",
    "ad_files = [f for f in files if '_ad' in f]\n",
    "healthy_files = [f for f in files if '_ad' not in f]\n",
    "\n",
    "random.seed(1)\n",
    "random.shuffle(ad_files)\n",
    "random.shuffle(healthy_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705b33c-b4c2-46ce-921f-ae332e38bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_files(file_list):\n",
    "    n = len(file_list)\n",
    "    return file_list[:int(0.8*n)], file_list[int(0.8*n):int(0.9*n)], file_list[int(0.9*n):]\n",
    "\n",
    "ad_train, ad_val, ad_test = split_files(ad_files)\n",
    "hl_train, hl_val, hl_test = split_files(healthy_files)\n",
    "\n",
    "train_files = ad_train + hl_train\n",
    "val_files = ad_val + hl_val\n",
    "test_files = ad_test + hl_test\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def segment_subjects(file_list, folder, num_segments, min_len, max_len):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for file in file_list:\n",
    "        mat = scipy.io.loadmat(os.path.join(folder, file))\n",
    "        eeg = mat['transferred_EEG']['trial'][0,0][0,0]  # first trial\n",
    "        total_samples = eeg.shape[1]\n",
    "        for _ in range(num_segments):\n",
    "            length = random.randint(min_len, max_len)\n",
    "            start = random.randint(0, total_samples - length)\n",
    "            segment = eeg[:, start:start+length]\n",
    "            segments.append(segment)\n",
    "            labels.append(1 if '_ad' in file else 0)\n",
    "    return segments, labels\n",
    "\n",
    "bands = {'Delta': (1, 4), 'Theta': (4, 8), 'Alpha': (8, 13), 'Beta': (13, 30), 'Gamma': (30, 70)}\n",
    "\n",
    "def extract_features(eeg_data, fs):\n",
    "    from scipy.signal import welch\n",
    "    features = []\n",
    "    for seg in eeg_data:\n",
    "        ch_features = []\n",
    "        for ch in seg:\n",
    "            f, pxx = welch(ch, fs=fs, window='hamming', nperseg=512, noverlap=256, nfft=512)\n",
    "            total_power = np.trapz(pxx[(f >= 0.5) & (f <= 70)], f[(f >= 0.5) & (f <= 70)])\n",
    "            p_norm = pxx / np.sum(pxx)\n",
    "\n",
    "            band_feats = []\n",
    "            for band, (low, high) in bands.items():\n",
    "                band_power = np.trapz(pxx[(f >= low) & (f <= high)], f[(f >= low) & (f <= high)])\n",
    "                band_feats.append(band_power / total_power)\n",
    "\n",
    "            entropy = -np.sum(p_norm * np.log2(p_norm + np.finfo(float).eps))\n",
    "            mean_freq = np.sum(f * pxx) / np.sum(pxx)\n",
    "            theta_alpha = band_feats[1] / (band_feats[2] + np.finfo(float).eps)\n",
    "            delta_alpha = band_feats[0] / (band_feats[2] + np.finfo(float).eps)\n",
    "\n",
    "            ch_features.append(band_feats + [entropy, mean_freq, theta_alpha, delta_alpha])\n",
    "\n",
    "        features.append(np.array(ch_features).flatten())\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc8499-7ba9-498d-b87f-613bd0ec410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Process Data ---\n",
    "train_data, train_labels = segment_subjects(train_files, data_folder, num_segments, min_samples, max_samples)\n",
    "val_data, val_labels = segment_subjects(val_files, data_folder, num_segments, min_samples, max_samples)\n",
    "test_data, test_labels = segment_subjects(test_files, data_folder, num_segments, min_samples, max_samples)\n",
    "\n",
    "# Balance train set\n",
    "train_data, train_labels = shuffle(train_data, train_labels, random_state=1)\n",
    "ad_idx = [i for i, l in enumerate(train_labels) if l == 1]\n",
    "hl_idx = [i for i, l in enumerate(train_labels) if l == 0]\n",
    "min_class = min(len(ad_idx), len(hl_idx))\n",
    "sel_idx = ad_idx[:min_class] + hl_idx[:min_class]\n",
    "train_data = [train_data[i] for i in sel_idx]\n",
    "train_labels = [train_labels[i] for i in sel_idx]\n",
    "\n",
    "# Feature extraction\n",
    "train_features = extract_features(train_data, fs)\n",
    "val_features = extract_features(val_data, fs)\n",
    "test_features = extract_features(test_data, fs)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "# --- Train Model ---\n",
    "model = MLPClassifier(hidden_layer_sizes=(64, 32), alpha=0.015, learning_rate_init=2e-4,\n",
    "                     max_iter=500, batch_size=32, early_stopping=True, random_state=1)\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# --- Evaluation ---\n",
    "predicted_labels = model.predict(test_features)\n",
    "accuracy = np.mean(predicted_labels == test_labels) * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "C = confusion_matrix(test_labels, predicted_labels)\n",
    "sns.heatmap(C, annot=True, fmt='d', xticklabels=['Healthy', 'AD'], yticklabels=['Healthy', 'AD'])\n",
    "plt.title(\"Confusion Matrix (Raw)\")\n",
    "plt.show()\n",
    "\n",
    "C_percent = 100 * C / C.sum(axis=1, keepdims=True)\n",
    "sns.heatmap(C_percent, annot=True, fmt='.1f', cmap='viridis', xticklabels=['Healthy', 'AD'], yticklabels=['Healthy', 'AD'])\n",
    "plt.title(\"Confusion Matrix (%)\")\n",
    "plt.show()\n",
    "\n",
    "# --- ROC Curve ---\n",
    "scores = model.predict_proba(test_features)[:,1]\n",
    "fpr, tpr, _ = roc_curve(test_labels, scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve'); plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Additional Metrics ---\n",
    "print(classification_report(test_labels, predicted_labels, target_names=['Healthy', 'AD']))\n",
    "\n",
    "# --- PCA and t-SNE Visualization ---\n",
    "print(\"Visualizing features with PCA and t-SNE...\")\n",
    "pca = PCA(n_components=2)\n",
    "pca_proj = pca.fit_transform(train_features)\n",
    "plt.figure()\n",
    "sns.scatterplot(x=pca_proj[:,0], y=pca_proj[:,1], hue=train_labels, palette='deep')\n",
    "plt.title('PCA Projection of Features')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "tsne_proj = TSNE(n_components=2, perplexity=30, random_state=1).fit_transform(train_features)\n",
    "plt.figure()\n",
    "sns.scatterplot(x=tsne_proj[:,0], y=tsne_proj[:,1], hue=train_labels, palette='deep')\n",
    "plt.title('t-SNE Projection of Features')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efb0f9-a2b7-468d-b511-da1f9c3089d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e8b63-ad5f-4e0b-a3da-5ee3cb67d417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f41a2-83d6-4761-9384-26c5e9936255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7860369-59b6-43f8-b3e9-f235bca60c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e3c7b-c86b-44f7-bd1a-d68e5ab46b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
